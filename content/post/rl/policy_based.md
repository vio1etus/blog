---
title: Policy-based Method of RL
comments: true
toc: true
tags:
description: æœ¬æ–‡å­¦ä¹  RL çš„ Policy-based Method
categories:
    - rl
date: 2020-12-17 16:35:39
tags:
---

Policy function ğœ‹(ğ‘|ğ‘ ) ç”¨æ¥æŒ‡å¯¼ agent å»è¿åŠ¨ï¼Œå®ƒæ¥å—ä¸€ä¸ªçŠ¶æ€ s ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡ï¼Œagent ä»æ‰€æœ‰åŠ¨ä½œä¸­é‡‡æ ·é€‰å–ä¸€ä¸ªåŠ¨ä½œ a æ‰§è¡Œã€‚

Can we directly learn a policy function ğœ‹(ğ‘|ğ‘ )?

-   If there are only a few states and actions, then yes, we can.
-   Draw a table (matrix) and learn the entries.
-   What if there are too many (or infinite) states or actions?

## Policy Network ğœ‹(ğ‘|ğ‘ ;ğ›‰)

è¿‘ä¼¼å‡½æ•°å¸¸ç”¨çš„æ˜¯çº¿æ€§å›å½’å’Œç¥ç»ç½‘ç»œã€‚

Policy network: Use a neural net to approximate ğœ‹(ğ‘|ğ‘ ;ğ›‰).

-   Use policy network ğœ‹(ğ‘|ğ‘ ;ğ›‰) to approximate ğœ‹(ğ‘|ğ‘ ;ğ›‰).
-   ğ›‰: trainable parameters of the neural net

$\sum_{a \in \mathcal{A}} \pi\left(\left.a\right|{s} ; \boldsymbol{\theta}\right)=1$

## State-value function

$V_{\pi}\left(s_{t}\right)=\mathbb{E}_{A}\left[Q_{\pi}\left(s_{t}, A\right)\right]=\sum_{a} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right)$

### Approximate state-value function

-   Approximate policy function $\pi(a|s_{t})$ by policy network $\pi(a|s_{t};\theta)$.
-   Approximate value function $V_{\pi}\left(s_{t}\right)$ by: $V\left(s_{t} ; \boldsymbol{\theta}\right)=\sum_{a} \pi\left(a \mid s_{t} ; \boldsymbol{\theta}\right) \cdot Q_{\pi}\left(s_{t}, a\right)$

Policy-based learning: Learn ğ›‰ that maximizes $J(\boldsymbol{\theta})=\mathbb{E}_{S}[V(S ; \boldsymbol{\theta})]$

Policy gradient ascent to improve ğ›‰:

-   Observe state s
-   Update policy by: $\theta \leftarrow \theta + \beta \cdot \frac{\partial V(s;\theta)}{\partial \theta}$
-   Policy gradient: $\frac{\partial V(s;\theta)}{\partial \theta}$

## Policy gradient

![form_1](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216104659.png)

![form_2](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216104712.png)

### Calculate Policy Gradient for Discrete Actions

![discrete_action](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216105014.png)

### Calculate Policy Gradient for Continuous Actions

![continuous_action](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216105826.png)

### Update policy network using policy gradient

![algorithm](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216110804.png)

Two Options:

![options](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216133540.png)

## Summary

![summary](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201216133836.png)
