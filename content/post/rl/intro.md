---
title: Introduction of RL
comments: true
toc: true
tags:
description: æœ¬æ–‡å­¦ä¹  RL çš„ Policy-based Method
categories:
    - rl
date: 2020-12-17 16:35:39
tags:
---

## Math

ä¸»è¦ä»‹ç»éœ€è¦ç”¨åˆ°çš„æ¦‚ç‡è®ºçš„åŸºç¡€çŸ¥è¯†ä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸­å…³é”®æœ¯è¯­ã€‚

1. Random Variable

    é€šå¸¸ä½¿ç”¨å¤§å†™å­—æ¯ï¼ˆå¦‚ï¼šXï¼‰è¡¨ç¤ºéšæœºå˜é‡, ä½¿ç”¨å°å†™å­—æ¯ï¼ˆå¦‚ï¼šx, x1ï¼‰è¡¨ç¤ºéšæœºå˜é‡çš„è§‚æµ‹å€¼ï¼ˆobserved valueï¼‰ï¼Œè§‚æµ‹å€¼æ˜¯ç¡®å®šçš„æ•°ï¼Œæ²¡æœ‰éšæœºæ€§ã€‚

2. Probability Density Functionï¼ˆPDFï¼‰

    æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼šPDF provides a relative likelihood that the value of the random variable would equal that sample.

    ä¾‹å¦‚ï¼šè¿ç»­çš„æ­£æ€åˆ†å¸ƒï¼ˆä¹Ÿå«åšé«˜æ–¯åˆ†å¸ƒï¼‰

    ![image-20201214155842957](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/image-20201214155842957.png)

    ç¦»æ•£çš„åˆ†å¸ƒï¼š

    ![image-20201214155924736](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/image-20201214155924736.png)

    å‡è®¾éšæœºå˜é‡ X çš„åŸŸä¸º $\mathbb{X}$

    1. è¿ç»­åˆ†å¸ƒ

        $\int_\mathbb{X} p(x)dx = 1$

        æœŸæœ›ï¼š $\mathbb{E}[f(x)] = \int_{x\in \mathbb{X}}{p(x) \cdot f(x)}dx$

    2. ç¦»æ•£åˆ†å¸ƒ

        $\sum_{x \in \mathbb{X}}{p(x)} = 1$

        æœŸæœ›ï¼š$\mathbb{E}[f(x)] = \sum_{x\in \mathbb{X}}{p(x) \cdot f(x)}$

3. Random Sampling

    éšæœºé‡‡æ ·ï¼š1. çŸ¥é“å„æƒ…å†µç§ç±»ä»¥åŠæ€»æ•°ï¼Œå»æŠ½æ · 2. çŸ¥é“å„æƒ…å†µæ¦‚ç‡ï¼Œå»æŠ½æ ·ã€‚

## Terminology

1. state and action
2. policy

    Policy function $\pi$ æ˜¯åŠ¨ä½œ A çš„æ¦‚ç‡å¯†åº¦å‡½æ•°: $\pi(a|s) = P(A=a|S=s)$
    å®ƒæ˜¯åœ¨æŸçŠ¶æ€ s ä¸‹ï¼Œé‡‡å–æŸä¸ªç‰¹å®šåŠ¨ä½œï¼ˆä¾‹å¦‚ï¼šA=a1ï¼‰çš„æ¦‚ç‡ã€‚
    ä¾‹å¦‚ï¼šç»™å®šçŠ¶æ€ sï¼ŒåŠ¨ä½œ A ç¦»æ•£åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š

    - $\pi(left|s) = 0.2$
    - $\pi(right|s) = 0.2$
    - $\pi(up|s) = 0.2$
    - $\pi(down|s) = 0.2$
      æ ¹æ®é€‰å–çš„ç­–ç•¥ä¸åŒï¼ˆéšæœºæ€§ç­–ç•¥æˆ–è€…ç¡®å®šæ€§ç­–ç•¥ï¼‰ï¼Œä»ä¸­éšæœºé€‰å–æˆ–è€…é€‰å–æ¦‚ç‡æœ€å¤§çš„ action è¿›è¡Œå®æ–½ã€‚

3. reward
4. state transition

    $old state \xrightarrow{action} new state$

    - State transation can be random
    - Randomness is from the environment(çŠ¶æ€è½¬ç§»çš„éšæœºæ€§å–å†³äºç¯å¢ƒï¼Œè¿™é‡Œçš„ç¯å¢ƒå°±æ˜¯æ¸¸æˆç¨‹åº)
    - çŠ¶æ€è½¬ç§»ï¼Œç”¨ p å‡½æ•°è¡¨ç¤ºï¼š $p(s^{'}|s,a) = \mathbb{P}(s^{'}=s,A=a)$, å¦‚æœè§‚å¯Ÿåˆ°å½“å‰çŠ¶æ€ s å’ŒåŠ¨ä½œ aï¼ŒP å‡½æ•°è¾“å‡ºçŠ¶æ€å˜ä¸º $p(s^{'}$ çš„æ¦‚ç‡ã€‚

5. agent environment interaction

    ![20201214194035](https://blog-1259556217.cos.ap-chengdu.myqcloud.com/image/20201214194035.png)

### Randomness in Reinforcement learning

1. Actions have randomness

    Given state ğ‘ , the action can be random

2. State transitions have randomness

    Given state ğ‘† = ğ‘  and action ğ´ = ğ‘, the environment randomly generates a new state $ğ‘†^{'}$

### Rewards and Returns

### Return

Definition: Return (aka cumulative future reward)
$ğ‘ˆ_{t} = ğ‘…_{t} + ğ‘…_{t+1} + ğ‘…_{t+2} + ğ‘…_{t+3} + â‹¯$

-   Future reward is less valuable than present reward.
-   $R_{t+1}$ should be given less weight than $ğ‘…_{t}$

### Discounted return

Definition: **Discounted** return (aka cumulative **discounted** future reward)

-   $\gamma$: discount rate (tuning hyper-parameter)
-   $ğ‘ˆ_{t} = ğ‘…_{t} + \gammağ‘…*{t+1} + \gamma^2ğ‘…*{t+2} + \gamma^3ğ‘…_{t+3} + â‹¯$

At time step ğ‘¡, the return $ğ‘ˆ_{t}$ is random.
Two sources of randomness:

1. Action can be random: $â„™[ğ´=ğ‘|ğ‘†=ğ‘ ]=ğœ‹(ğ‘|ğ‘ )$
2. New state can be random: $â„™[ğ‘†^{'}=ğ‘ ^{'}|ğ‘†=ğ‘ ,ğ´=ğ‘]=ğ‘(ğ‘ ^{'}|ğ‘ ,ğ‘)$
    - For any ğ‘– â‰¥ ğ‘¡, the reward ğ‘…n depends on ğ‘†n and ğ´n.
    - Thus, given ğ‘ *, the return ğ‘ˆ* depends on the random variables:
    - $ğ´_{t},ğ´_{t+1},ğ´_{t+2},â‹¯$ and $ğ‘†_{t+1},ğ‘†_{t+2},â‹¯$

### Action-Value Function ğ‘„(s,a)

Definition: Action-value function for policy ğœ‹

$ğ‘„_{\pi}(ğ‘ _{t},ğ‘_{t}) = \mathbb{E}[ğ‘ˆ_{t}|ğ‘†_{t}=ğ‘ _{t}, ğ´_{t}=ğ‘_{t}]$

-   Return $ğ‘ˆ_{t}$ depends on states $ğ‘†_{t},ğ‘†_{t+1},S_{t+2},â‹¯$ and actions $A_{t},A_{t+1},A_{t+2},â‹¯$
-   å°†éšæœºå˜é‡ $ğ‘ˆ_{t}$ å½“ä½œæœªæ¥æ‰€æœ‰çŠ¶æ€ S ä»¥åŠæœªæ¥æ‰€æœ‰åŠ¨ä½œ A çš„ä¸€ä¸ªå‡½æ•°æ±‚æœŸæœ›ï¼Œå°†è¿™äº›å˜é‡ç§¯åˆ†ç§¯æ‰ï¼Œå¾—åˆ°ä¸€ä¸ªæ•° $ğ‘„_{\pi}$ã€‚
-   $ğ‘†_{t}, A_{t}$ è¢«ä½œä¸ºè§‚æµ‹åˆ°çš„æ•°å€¼è€Œå¯¹å¾…ï¼Œæ²¡æœ‰è¢«ç§¯æ‰äº†ã€‚å› æ­¤ $ğ‘„_{\pi}$ çš„å€¼ä¸ $ğ‘†_{t}, A_{t}$ æœ‰å…³ã€‚
-   ç§¯åˆ†çš„æ—¶å€™ä¼šç”¨åˆ° Policy å‡½æ•°ï¼Œå¦‚æœ Policy å‡½æ•° $\pi$ ä¸ä¸€æ ·ï¼Œç§¯åˆ†å¾—åˆ°çš„å‡½æ•° $ğ‘„_{\pi}$ å°±ä¸ä¸€æ ·ã€‚

æœªæ¥çŠ¶æ€å’ŒåŠ¨ä½œéƒ½æœ‰éšæœºæ€§

-   Actions are random: $\mathbb{P}[A=a|S=s]=\pi(a|s)$, åŠ¨ä½œ A çš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ Policy function $\pi$
-   States are random: $\mathbb{P}[S^{'}=s^{'}|A=a]=p(s^{'}|s,a)$, çŠ¶æ€ S çš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ state transition function p.

$ğ‘„_{\pi}$ çš„æ„ä¹‰ï¼šé‡‡å–ç­–ç•¥ $\pi$ï¼Œåœ¨ $S_{t}$ çŠ¶æ€ä¸‹ï¼ŒåŠ¨ä½œ $A_{t}$ çš„å¥½åã€‚

### Optimal action-value function

ç”¨ä¸åŒçš„ç­–ç•¥ $\pi$ï¼Œ å°±æœ‰ä¸åŒçš„ $ğ‘„_{\pi}$ï¼Œ é‚£å¦‚ä½•å°† Action-Value function ä¸­çš„ $\pi$ å»æ‰å‘¢ï¼Ÿç­”æ¡ˆæ˜¯ï¼š $ğ‘„_{\pi}$ å…³äº $\pi$ æ±‚æœ€å¤§åŒ–ã€‚

Definition: Optimal action-value function.

$Q^{\star}\left(s_{t}, a_{t}\right)=\underset{\pi}{\max} Q_{\pi}\left(s_{t}, a_{t}\right)$

ä½¿ç”¨æœ€å¥½çš„ç­–ç•¥ $\pi$ï¼ˆä¹Ÿå°±æ˜¯ä½¿å¾— $ğ‘„_{\pi}$ å‡½æ•°æœ€å¤§åŒ–çš„é‚£ä¸ª $\pi$ï¼‰ï¼Œå› æ­¤ $Q^{\star}$ ä¸ $\pi$ æ— å…³äº†ï¼Œå®ƒåªä¸ $S_{t}, A_{t}$ æœ‰å…³ã€‚

$Q^{\star}$ çš„æ„ä¹‰ï¼šåœ¨æŸä¸ªçŠ¶æ€ s ä¸‹ï¼Œå¯¹åŠ¨ä½œ a åšè¯„ä»·ã€‚

### State-value function

-   $V_{\pi}(s_{t}) = \mathbb{E_{A}}[Q_{\pi}(s_{t}, A)] = \sum{\pi(a|s_{t})\cdot Q_{\pi}(s_{t},a)}$ (Actions are discrete)
-   $V_{\pi}(s_{t}) = \mathbb{E_{A}}[Q_{\pi}(s_{t}, A)] = \int{\pi(a|s_{t})\cdot Q_{\pi}(s_{t},a)}da$ (Actions are continuous)

å°†åŠ¨ä½œ A å½“ä½œéšæœºå˜é‡ï¼Œå¯¹å…¶è¿›è¡Œç§¯åˆ†æ±‚æœŸæœ›ï¼Œ å…¶ä¸­ A çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $A~\pi(\cdot|s_{t})$ï¼Œå°† A æ¶ˆæ‰ï¼Œç„¶å $V_{\pi}$ å‡½æ•°å°±åªä¸ $\pi$ å’Œ S æœ‰å…³ã€‚

æ„ä¹‰ï¼š$V_{\pi}$ å¯ä»¥å‘Šè¯‰æˆ‘ä»¬å½“å‰çš„å±€åŠ¿çŠ¶å†µå¥½åä¼˜åŠ£ã€‚

### Value Functions

1. Action-value function

    $ğ‘„_{\pi}(ğ‘ _{t},ğ‘_{t}) = \mathbb{E}[ğ‘ˆ_{t}|ğ‘†_{t}=ğ‘ _{t}, ğ´_{t}=ğ‘_{t}]$

    For policy $\pi$, $ğ‘„_{\pi}(ğ‘ _{t},ğ‘_{t})$ evaluates how good it is for an agent to pick action **a** while being in state **s**.

2. State value function

    $V_{\pi}(s*{t}) = \mathbb{E_{A}}[Q_{\pi}(s_{t}, A)] = \sum_{a} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right)$

    å¦‚æœåŠ¨ä½œ A çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $\pi$ æ˜¯ç¦»æ•£çš„,åˆ™ $V_{\pi}$ å¯ä»¥è¿›ä¸€æ­¥å†™æˆæ±‚å’Œçš„å½¢å¼ï¼›å¦‚æœæ˜¯è¿ç»­çš„ï¼Œåˆ™å¯å†™ä¸ºç§¯åˆ†å½¢å¼ã€‚

    For fixed policy $\pi$, $V_{\pi}(s_{t})$ evaluates how good the situation is in state **s**.

    $\mathbb{E}_{s}[V_{\pi}(S)]$ evaluates how good the policy $\pi$ is.

## Two Methods

1. Policy-basedï¼šSuppose we have a good policy $\pi(a|s)$.

    - Upon observing the state $S_{t}$
    - random sampling $a_{t}~\pi(\cdot|s_{t})$

2. Value-basedï¼šSuppose we know the optimal action-value function $Q^{\star}(s|a)$

    - Upon observe the state $S_{t}$
    - choose the **action** that maximizes the value $a_{t} = argmax_{a}Q^{\star}(s_{t},a)$

## å‡ ç§åˆ†ç±»

| é€šè¿‡ä»·å€¼é€‰è¡Œä¸º | ç›´æ¥é€‰è¡Œä¸º      | æƒ³è±¡ç¯å¢ƒå¹¶ä»ä¸­å­¦ä¹  |
| -------------- | --------------- | ------------------ |
| Q learing      | Policy Gradient | Model based Rl     |
| Sarsa          |                 |                    |
| Deep Q Network |                 |                    |

### ç†è§£ç¯å¢ƒ

æ ¹æ®ç†è§£ç¯å¢ƒä¸å¦ï¼Œ åˆ†ä¸º Model-Free RL ä¸ Model-Based RL

1. Model-Free

    ç‰¹ç‚¹ï¼šä¸å°è¯•ç†è§£ç¯å¢ƒï¼Œæ ¹æ®ç¯å¢ƒçš„åé¦ˆä¸€æ­¥æ­¥å­¦ä¹ 
    ä»£è¡¨ï¼šQ Learningã€Sarsaã€Policy Gradients

2. Mode-Based

    ç‰¹ç‚¹ï¼šå»ºæ¨¡ï¼Œå¯ä»¥è‡ªå·±è®­ç»ƒ

### æ¦‚ç‡ä¸ä»·å€¼

åŸºäºæ¦‚ç‡(Policy-Based RL)ã€åŸºäºä»·å€¼(Value-Based RL)

1. åŸºäºæ¦‚ç‡(Policy-Based RL)

    æ ¹æ®æ‰€å¤„çš„ç¯å¢ƒï¼Œåˆ¤æ–­ä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„æ¦‚ç‡ï¼Œæ¯ä¸€ç§åŠ¨ä½œéƒ½æœ‰å¯èƒ½è¢«é€‰ä¸­ï¼Œåªæ˜¯å¯èƒ½æ€§å¤§å°ä¸åŒ
    ä½¿ç”¨è¿ç»­çš„æ¦‚ç‡åˆ†å¸ƒå¤„ç†é€‰æ‹©è¿ç»­çš„åŠ¨ä½œ
    ä»£è¡¨ï¼šPolicy Gradients

2. åŸºäºä»·å€¼(Value-Based RL)

    ç›´æ¥é€‰æ‹©ä»·å€¼æœ€é«˜çš„åŠ¨ä½œï¼›æ— æ³•å¤„ç†è¿ç»­çš„åŠ¨ä½œ
    ä»£è¡¨ï¼šQ Learningã€Sarsa

3. ç»“åˆ Policy å’Œ Value çš„ Actor-Critic

    Actorï¼š åŸºäºæ¦‚ç‡åšå‡ºåŠ¨ä½œ
    Criticï¼š æ ¹æ®åšå‡ºçš„åŠ¨ä½œç»™å‡ºä»·å€¼

### æ›´æ–°é¢‘ç‡

1. å›åˆæ›´æ–°(Monte-Carlo update)
   ç­‰å¾…æ¯æ¬¡è®­ç»ƒç»“æŸï¼Œè¿›è¡Œæ›´æ–°
   ä»£è¡¨ï¼šåŸºç¡€ç‰ˆ Policy Gradientsã€Monte-Carlo Learing

2. å•æ­¥æ›´æ–°(Temporal-Difference update)
   è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥éƒ½è¿›è¡Œæ›´æ–°
   ä»£è¡¨ï¼šQ Learningã€Sarsaã€å‡çº§ç‰ˆ Policy Gradients

### åœ¨çº¿ä¸ç¦»çº¿

1. åœ¨çº¿å­¦ä¹ 
   æœ¬äººåœ¨åœºï¼Œè¾¹ç©è¾¹å­¦ä¹ 
   ä»£è¡¨ï¼šSarsaã€Sarsa(Lambda)

2. ç¦»çº¿å­¦ä¹ 
   å¯ä»¥ä»ä»–äººçš„ç»å†ä¸­è¿›è¡Œå­¦ä¹ 
   ä¹Ÿå¯ä»¥ç™½å¤©å­˜å‚¨å­¦ä¹ ç»éªŒï¼Œæ™šä¸Šç¦»çº¿å­¦ä¹ 
   ä»£è¡¨ï¼šQ Learningã€Deep Q Network

## Going to learn

1. Value-based learning.

    - Deep Q network (DQN) for approximating $ğ‘„^{\star}(ğ‘ |ğ‘)$.
    - Learn the network parameters using temporal different(TD).

2. Policy-based learning.

    - Policy network for approximating ğœ‹(ğ‘|ğ‘ ).
    - Learn the network parameters using **policy gradient**.

3. Actor-critic method. (Policy network + value network.)
4. Example: AlphaGo

## èµ„æ–™

1. 2015 å¹´ David Silver çš„ç»å…¸è¯¾ç¨‹[Teaching](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)
2. 2017 å¹´åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ Levine, Finn, Schulman çš„è¯¾ç¨‹ [CS 294 Deep Reinforcement Learning, Spring 2017](http://rll.berkeley.edu/deeprlcourse/)
3. å¡å†…åŸºæ¢…éš†å¤§å­¦çš„ 2017 æ˜¥å­£è¯¾ç¨‹[Deep RL and Control](https://katefvision.github.io/)ã€‚
