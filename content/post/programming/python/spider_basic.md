---
title: 爬虫基础
comments: true
toc: true
description: 本文总结一些爬虫的预备知识，以便后续爬虫的学习
summary: 本文总结一些爬虫的预备知识，以便后续爬虫的学习
tags:
    - python
categories:
    - programming
date: 2019-09-15 22:16:23
---

## HTTP 常见请求头

-   Host（主机和端口号）

-   Connection（链接类型）

-   Upgrade-Insecure-Requests（值为 1 ，则升级为 HTTPS 请求）

-   **User-Agent（浏览器名称）**

-   Accept（传输文件类型）

-   Referer（页面跳转处）

-   Accept-Encoding（文件编解码格式）

-   **Cookie （Cookie）**

-   X-requested-with: XMLHttpRequest（是 Ajax 异步请求）

-   X-forward-for: ip0 ip1 ip2 表示 HTTP 请求端真实 IP 为 ip0\*\*

-   Remote Address: 最终请求服务器地址
-   Aceept：多种类型，每一种/几种类型后面的 q 是权重的意思，权重大，优先级高

requests 模块在 Python 2 和 Python 3 中都是一样的，因此不用担心。

## User-Agent

1. 可以在 chrome 的开发者工具中，更改为各种手机的 UA。有时候电脑版的网页的一些地址、post 数据发向的 URL 不好找或加密繁杂有时间戳等，可以切换到手机版看看，往往有惊喜哦。
2. 可以通过切换 UA 来降低被识别为爬虫的几率

## Cookie 与 Seesion

cookie 保存在浏览器本地，用户/本地可以修改其内容。因为它是保存在本地的，所以它的大小是有上限的

seesion 也保存在服务器端，用户/本地无法更改其内容。因为它是保存在服务器上面的，所以它的大小取决于服务器大小。

## 字符串区别与转化

Python 3 中字符串分为以下两种类型：str 类型和 bytes 类型

bytes 类型：**二进制**

-   互联网上的数据都是以二进制的方式来传输的

str 类型：unicode 的呈现形式

编码方式解码方式必须一样，否则就会出现乱码

## 字符编码简单回顾

**Unicode UTF8 ASCII 的补充**
字符集包括：ASCI 字符集、GB2312 字符集、GB18030 字符集、Unicode 字符集等
ASCll 编码是 1 个字节，而 Unicode 编码**通常**是 2 个字节。
UTF-8 是 Unicode 的实现方式之一，UTF-8 是它是一种变长的编码方式，可以是 1，2，3 个
字节

# 爬虫的定义

网络爬虫（又被称为网页蜘蛛，网络机器人）就是模拟客户端发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。
**只要是浏览器能做的事情，原则上，爬虫都能够做**

要爬取一个网站的数据，不一定要硬怼这个网站本身。我们可以换个渠道：

1. 改换 UA，手机版等
2. 去与该网站有合作的网站，迂回操作

# 爬虫分类

1. 通用爬虫：通用爬虫是搜索引擎抓取系统（百度、谷歌、搜狗等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。
2. 聚焦爬虫：是面向**特定需求**的一种网络爬虫程序，他与通用爬虫的区别在于：聚焦爬虫在实施网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。

# 通用搜索引擎的工作原理

## 搜索引擎的流程

抓取网页 ----> 数据存储 ----> 预处理 ----> 提供检索服务、网站排名（点击率，引用次数等）

种子 URL 地址不一定是网上的所有 URL 地址，只要通过某个 URL 的响应内容中的 URL 可以达到就行

搜索引擎排名算法：PageRank 算法

交换友链以及要求在文章或网站中放链接的目的：

为了增加网站或文章被引用的次数，当百度引擎获取一个网页内容时，也可以获取里面的 URL，从而提高里面 URL 的曝光率。而且如果文章等被权重大的网站引用，那么效果更好。

### robots 协议

robots 协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。

robots 协议也叫 robots.txt（统一小写）是一种存放于网站根目录下的 ASCII 编码的文本文件，它通常告诉网络搜索引擎的爬虫，此网站中的哪些内容是不应被搜索引擎的爬虫获取的，哪些是可以被爬虫获取的。

# 聚焦爬虫

URL List ----> 响应内容（根据响应内容提取 URL ，放入 URL List 再次提取） ----> 提取数据 ----> 入库

在相应内容中提取 URL， 可以使用

-   下一页处获取 URL，直到没有下一页

-   详情处提取 URL

学习规划：

基础知识 ----> requests 使用 ----> 数据提取方法 ----> 动态网页提取数据 ----> scrapy ----> scrapy redis

**浏览器渲染出来的页面和爬虫请求的页面并不一样的。**

爬虫请求的只有 HTML，而浏览器会根据 html 中的 js，图片等地址，请求图片，css 等，同时 js 还会动态改变页面，最终渲染出页面。

Chrome 中的 Elements 与 Network 中的 Repsonse

network 中的 response 只是纯的 HTML，而 Elements 是包含 CSS, JS, 图片以及当前 URL 地址的响应（response）的，而当前页面可能会通过 JS， Ajax 动态生成、改变网页内容。

我们在后续爬虫时，要以当前 URL 地址对应的响应 response 为准，如果有一天我们根据当前 URL 地址对应的响应，拿不到数据，那可能在其他 URL 的响应里面或者是通过 JS 生成的。

爬虫的时候我们要确定 URL 中的参数哪些是有用的，哪些是没用的，有用的参数会不会变，如果变，那其规律是什么
